{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Survival on the Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.graph_objs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score, roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the color palette \n",
    "Viridis=[\n",
    "\"#440154\", \"#440558\", \"#450a5c\", \"#450e60\", \"#451465\", \"#461969\",\n",
    "\"#461d6d\", \"#462372\", \"#472775\", \"#472c7a\", \"#46307c\", \"#45337d\",\n",
    "\"#433880\", \"#423c81\", \"#404184\", \"#3f4686\", \"#3d4a88\", \"#3c4f8a\",\n",
    "\"#3b518b\", \"#39558b\", \"#37598c\", \"#365c8c\", \"#34608c\", \"#33638d\",\n",
    "\"#31678d\", \"#2f6b8d\", \"#2d6e8e\", \"#2c718e\", \"#2b748e\", \"#29788e\",\n",
    "\"#287c8e\", \"#277f8e\", \"#25848d\", \"#24878d\", \"#238b8d\", \"#218f8d\",\n",
    "\"#21918d\", \"#22958b\", \"#23988a\", \"#239b89\", \"#249f87\", \"#25a186\",\n",
    "\"#25a584\", \"#26a883\", \"#27ab82\", \"#29ae80\", \"#2eb17d\", \"#35b479\",\n",
    "\"#3cb875\", \"#42bb72\", \"#49be6e\", \"#4ec16b\", \"#55c467\", \"#5cc863\",\n",
    "\"#61c960\", \"#6bcc5a\", \"#72ce55\", \"#7cd04f\", \"#85d349\", \"#8dd544\",\n",
    "\"#97d73e\", \"#9ed93a\", \"#a8db34\", \"#b0dd31\", \"#b8de30\", \"#c3df2e\",\n",
    "\"#cbe02d\", \"#d6e22b\", \"#e1e329\", \"#eae428\", \"#f5e626\", \"#fde725\"]\n",
    "# source: https://bhaskarvk.github.io/colormap/reference/colormap.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Name</th>\n",
       "      <th>SibSp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age     Fare     Embarked  \\\n",
       "0         0       3    male  22.0   7.2500  Southampton   \n",
       "1         1       1  female  38.0  71.2833    Cherbourg   \n",
       "2         1       3  female  26.0   7.9250  Southampton   \n",
       "3         1       1  female  35.0  53.1000  Southampton   \n",
       "4         0       3    male  35.0   8.0500  Southampton   \n",
       "\n",
       "                                                Name  SibSp  \n",
       "0                            Braund, Mr. Owen Harris      1  \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...      1  \n",
       "2                             Heikkinen, Miss. Laina      0  \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)      1  \n",
       "4                           Allen, Mr. William Henry      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('resources/titanic.csv')\n",
    "# df = pd.read_csv(\"https://raw.githubusercontent.com/austinlasseter/plotly_dash_tutorial/master/00%20resources/titanic.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some dummies for sex\n",
    "df = pd.get_dummies(df, prefix='', prefix_sep='', columns=['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some dummies Passenger's cabin class\n",
    "df = pd.get_dummies(df, prefix='Cabin Class', prefix_sep=' ', columns=['Pclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some dummies Passenger's embarkation\n",
    "df = pd.get_dummies(df, prefix='', prefix_sep='', columns=['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    469\n",
       "1    183\n",
       "2     25\n",
       "4     18\n",
       "3     12\n",
       "5      5\n",
       "Name: Siblings and Spouses, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Siblings and Spouses\n",
    "df=df.rename(columns={'SibSp':'Siblings and Spouses'})\n",
    "df['Siblings and Spouses'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    712.000000\n",
      "mean      29.642093\n",
      "std       14.492933\n",
      "min        0.420000\n",
      "25%       20.000000\n",
      "50%       28.000000\n",
      "75%       38.000000\n",
      "max       80.000000\n",
      "Name: Age, dtype: float64\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "# age requires binning\n",
    "print(df.Age.describe())\n",
    "print(df.Age.describe()['25%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20]     165\n",
       "(20, 28]    183\n",
       "(28, 38]    174\n",
       "(38, 80]    176\n",
       "Name: age_binned, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Age\n",
    "bins=[1, 20, 28, 38, 80]\n",
    "df['age_binned']=pd.cut(df['Age'], bins)\n",
    "df['age_binned'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, prefix='Age', prefix_sep=' ', columns=['age_binned'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name\n",
    "df['Last Name']=df['Name'].apply(lambda x: x.split(',')[0])\n",
    "df['First Name']=df['Name'].apply(lambda x: x.split(',')[1])\n",
    "df['Title']=df['First Name'].apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title\n",
    "print(df['Title'].value_counts())\n",
    "df['Mr.']=np.where((df['Title']==' Mr')|(df['Title']==' Master')|(df['Title']==' Ms'), 1, 0) # Ms is actually monsieur\n",
    "df['Mrs.']=np.where((df['Title']==' Mrs')|(df['Title']==' Mme'), 1, 0)\n",
    "df['Miss']=np.where((df['Title']==' Miss')|(df['Title']==' Mlle'), 1, 0)\n",
    "df['VIP']=np.where((df['Mr.']==0)&(df['Mrs.']==0)&(df['Miss']==0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VIP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values as they will skew the regression\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the possible features?\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns out that fare doesn't add any significant or meaningful coefficient to the final model, so we're dropping it.\n",
    "feature_cols=list(df.columns)\n",
    "for item in ['Survived', 'Fare', 'Cabin Class 1', 'Southampton', 'male', 'Age',  'Age (1, 20]', 'Last Name', 'First Name', 'Title', 'Mr.']:\n",
    "    feature_cols.remove(item)\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our features\n",
    "X = df[feature_cols]\n",
    "y = df.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train1, X_test1, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the names, we'll save those for later use.\n",
    "X_train=X_train1.drop('Name', axis=1)\n",
    "X_test=X_test1.drop('Name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "# Fit on the training data\n",
    "gnb_model = gnb.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "predictions=gnb_model.predict(X_test)\n",
    "probabilities = gnb_model.predict_proba(X_test)[:,1]\n",
    "# Calculate the roc-auc score\n",
    "auc_nb=metrics.roc_auc_score(y_test, predictions)\n",
    "acc_nb = metrics.accuracy_score(y_test, predictions)\n",
    "f1_nb = metrics.f1_score(y_test, predictions)\n",
    "# Display\n",
    "print('F1 Score', \"%.4f\" % round(f1_nb,4))\n",
    "print('Accuracy', \"%.4f\" % round(acc_nb,4))\n",
    "print('AUC Score', \"%.4f\" % round(auc_nb,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "# Fit on the training data\n",
    "knn_model=knn.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "predictions=knn_model.predict(X_test)\n",
    "probabilities = knn_model.predict_proba(X_test)[:,1]\n",
    "# Calculate the roc-auc score\n",
    "auc_knn=metrics.roc_auc_score(y_test, predictions)\n",
    "acc_knn = metrics.accuracy_score(y_test, predictions)\n",
    "f1_knn = metrics.f1_score(y_test, predictions)\n",
    "# Display\n",
    "print('F1 Score', \"%.4f\" % round(f1_knn,4))\n",
    "print('Accuracy', \"%.4f\" % round(acc_knn,4))\n",
    "print('AUC Score', \"%.4f\" % round(auc_knn,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "# Fit on the training data\n",
    "rf_model=rf.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "predictions=rf_model.predict(X_test)\n",
    "probabilities = rf_model.predict_proba(X_test)[:,1]\n",
    "# Calculate the roc-auc score\n",
    "auc_rf=metrics.roc_auc_score(y_test, predictions)\n",
    "acc_rf = metrics.accuracy_score(y_test, predictions)\n",
    "f1_rf = metrics.f1_score(y_test, predictions)\n",
    "# Display\n",
    "print('F1 Score', \"%.4f\" % round(f1_rf,4))\n",
    "print('Accuracy', \"%.4f\" % round(acc_rf,4))\n",
    "print('AUC Score', \"%.4f\" % round(auc_rf,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "# Fit on the training data\n",
    "log_model=logreg.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "predictions=log_model.predict(X_test)\n",
    "probabilities = log_model.predict_proba(X_test)[:,1]\n",
    "# Calculate the roc-auc score\n",
    "auc_log=metrics.roc_auc_score(y_test, predictions)\n",
    "acc_log = metrics.accuracy_score(y_test, predictions)\n",
    "f1_log = metrics.f1_score(y_test, predictions)\n",
    "# Display\n",
    "print('F1 Score', \"%.4f\" % round(f1_log,4))\n",
    "print('Accuracy', \"%.4f\" % round(acc_log,4))\n",
    "print('AUC Score', \"%.4f\" % round(auc_log,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Four Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists from the metrics we produced.\n",
    "f1=[f1_nb, f1_log, f1_knn, f1_rf]\n",
    "acc=[acc_nb, acc_log, acc_knn, acc_rf]\n",
    "auc=[auc_nb, auc_log, auc_knn, auc_rf]\n",
    "# Define a function that will round our metrics.\n",
    "def rounder(metric):\n",
    "    scores_list=[]\n",
    "    for score in metric:\n",
    "        scores_list.append(round(float(score*100),1))\n",
    "    return scores_list\n",
    "# Apply it to each of the three lists.\n",
    "f1_scores=rounder(f1)\n",
    "acc_scores=rounder(acc)\n",
    "auc_scores=rounder(auc)\n",
    "score_types=['F1 score', 'Accuracy', 'AUC score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of model metrics\n",
    "models=['naive bayes', 'logistic regression', 'k-nearest neighbors', 'random forest']\n",
    "index=['F1 score', 'Accuracy', 'AUC score']\n",
    "compare_models=pd.DataFrame([f1_scores, acc_scores, auc_scores], index=index, columns=models)\n",
    "# save to pickle, for later use by plotly dash app.\n",
    "compare_models.to_pickle('resources/compare_models.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display that with plotly.\n",
    "mydata1 = go.Bar(\n",
    "    x=compare_models.loc['F1 score'].index,\n",
    "    y=compare_models.loc['F1 score'],\n",
    "    name=compare_models.index[0],\n",
    "    marker=dict(color=Viridis[50])\n",
    ")\n",
    "mydata2 = go.Bar(\n",
    "    x=compare_models.loc['Accuracy'].index,\n",
    "    y=compare_models.loc['Accuracy'],\n",
    "    name=compare_models.index[1],\n",
    "    marker=dict(color=Viridis[30])\n",
    ")\n",
    "mydata3 = go.Bar(\n",
    "    x=compare_models.loc['AUC score'].index,\n",
    "    y=compare_models.loc['AUC score'],\n",
    "    name=compare_models.index[2],\n",
    "    marker=dict(color=Viridis[10])\n",
    ")\n",
    "mylayout = go.Layout(\n",
    "    title='Comparison of Possible Models',\n",
    "    xaxis = dict(title = 'Predictive models'), # x-axis label\n",
    "    yaxis = dict(title = 'Score'), # y-axis label\n",
    "    \n",
    ")\n",
    "fig = go.Figure(data=[mydata1, mydata2, mydata3], layout=mylayout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Logistic Classifier\n",
    "Note: The gridsearch step is included here for completeness sake, as this is a smart inclusion in any iteration of possible models. But for the sake of speed (this notebook was run multiple times during development) I've kept my gridsearch to a bare-bones placeholder. A more complete project would use a broader grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularization penalty space (l1=ridge, l2=lasso)\n",
    "penalty = ['l1', 'l2'] \n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "grid_lr = GridSearchCV(LogisticRegression(), hyperparameters, cv=5,  n_jobs = 1, verbose=0)\n",
    "grid_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_lr.best_params_)\n",
    "\n",
    "log_model = grid_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the testing data\n",
    "\n",
    "predictions=log_model.predict(X_test)\n",
    "probabilities = log_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the final model for use in the plotly dash app.\n",
    "file = open('resources/final_logreg_model.pkl', 'wb')\n",
    "pickle.dump(log_model, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full list of metrics\n",
    "def model_metrics(y_test, predictions):\n",
    "    '''\n",
    "    Calculate 5 standard model metrics\n",
    "    Return a dictionary with the metrics\n",
    "    '''\n",
    "    f1 = metrics.f1_score(y_test, predictions)\n",
    "    accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "    error = 1 - accuracy\n",
    "    precision = metrics.precision_score(y_test, predictions)\n",
    "    recall = metrics.recall_score(y_test, predictions)\n",
    "    rocauc =  metrics.roc_auc_score(y_test, predictions)\n",
    "    return {'precision': precision, 'recall': recall,'f1 score':f1, 'accuracy': accuracy, 'error rate': error,  'ROC-AUC': rocauc}\n",
    "\n",
    "eval_scores=model_metrics(y_test, predictions)\n",
    "eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the y values.\n",
    "y_vals=[]\n",
    "for val in list(eval_scores.values()):\n",
    "    y_vals.append(round(float(val*100),1))\n",
    "y_vals    \n",
    "# Write over the previous dictionary with the rounded values.\n",
    "eval_scores=dict(zip(eval_scores.keys(), y_vals))\n",
    "print(eval_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save that dictionary to a pickle file, for later use in plotly dash app\n",
    "file = open('resources/eval_scores.pkl', 'wb')\n",
    "pickle.dump(eval_scores, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here's a reminder of how to read that back in again, just in case this is unfamiliar:\n",
    "file = open('resources/eval_scores.pkl', 'rb')\n",
    "evals=pickle.load(file)\n",
    "file.close()\n",
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert that into a visualization.\n",
    "mydata = [go.Bar(\n",
    "    x=list(evals.keys()),\n",
    "    y=list(evals.values()),\n",
    "    marker=dict(color=Viridis[::12])\n",
    ")]\n",
    "\n",
    "mylayout = go.Layout(\n",
    "    title='Evaluation Metrics for Logistic Regression Model (Testing Dataset = 127 passengers)',\n",
    "    xaxis = {'title': 'Metrics'},\n",
    "    yaxis = {'title': 'Percent'}, \n",
    "\n",
    ")\n",
    "fig = go.Figure(data=mydata, layout=mylayout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR, TPR, _ = roc_curve(y_test, probabilities)\n",
    "FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_score=round(100*roc_auc_score(y_test, predictions),1)\n",
    "roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle everything we need to reproduce the ROC-AUC figure in plotly dash.\n",
    "file = open('resources/FPR.pkl', 'wb')\n",
    "pickle.dump(FPR, file)\n",
    "file.close()\n",
    "\n",
    "file = open('resources/TPR.pkl', 'wb')\n",
    "pickle.dump(TPR, file)\n",
    "file.close()\n",
    "\n",
    "file = open('resources/y_test.pkl', 'wb')\n",
    "pickle.dump(y_test, file)\n",
    "file.close()\n",
    "\n",
    "file = open('resources/predictions.pkl', 'wb')\n",
    "pickle.dump(predictions, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC figure\n",
    "\n",
    "roc_score=round(100*roc_auc_score(y_test, predictions),1)\n",
    "trace0=go.Scatter(\n",
    "        x=FPR, \n",
    "        y=TPR,\n",
    "        mode='lines',\n",
    "        name=f'AUC: {roc_score}',\n",
    "        marker=dict(color=Viridis[10])\n",
    "        )\n",
    "trace1=go.Scatter(\n",
    "        x=[0,1], \n",
    "        y=[0,1],\n",
    "        mode='lines',\n",
    "        name='Baseline Area: 50.0',\n",
    "    marker=dict(color=Viridis[50])\n",
    "        )\n",
    "layout=go.Layout(\n",
    "    title='Receiver Operating Characteristic (ROC): Area Under Curve',\n",
    "    xaxis={'title': 'False Positive Rate (100-Specificity)','scaleratio': 1,'scaleanchor': 'y'},\n",
    "    yaxis={'title': 'True Positive Rate (Sensitivity)'}\n",
    "    )\n",
    "data=[trace0, trace1]\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix tells us our false positives and false negatives:\n",
    "matrix=confusion_matrix(y_test, predictions)\n",
    "print(matrix)\n",
    "cm=pd.DataFrame(matrix, columns=['pred: survival', 'pred: death'])\n",
    "cm[f'n={len(y_test)}']=['actual: survival', 'actual: death']\n",
    "cm=cm[[f'n={len(y_test)}', 'pred: survival', 'pred: death']]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cm dataframe to a pickle file, for later use in plotly dash app\n",
    "cm.to_pickle('resources/confusion_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix as a formatted table with Plotly\n",
    "trace = go.Table(\n",
    "    header=dict(values=cm.columns,\n",
    "                line = dict(color='#7D7F80'),\n",
    "                fill = dict(color=Viridis[55]),\n",
    "                align = ['left'] * 5),\n",
    "    cells=dict(values=[cm[f'n={len(y_test)}'], cm['pred: survival'], cm['pred: death']],\n",
    "               line = dict(color='#7D7F80'),\n",
    "               fill = dict(color='white'),\n",
    "               align = ['left'] * 5))\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = f'Confusion Matrix: Logistic Regression Model (Testing Dataset)'\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (Logistic Regression)\n",
    "coeffs1=pd.DataFrame(list(zip(list(X_train.columns), logreg.coef_[0])), columns=['feature', 'coefficient'])\n",
    "coeffs=coeffs1.sort_values(by='coefficient', ascending=False)\n",
    "\n",
    "# Format the coefficients.\n",
    "y_vals=[]\n",
    "for val in list(coeffs['coefficient']):\n",
    "    y_vals.append(round(float(val),2))\n",
    "y_vals\n",
    "\n",
    "coeffs['coefficient']=y_vals\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv file, for later use by plotly dash app.\n",
    "coeffs.to_pickle('resources/coefficients.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display that with Plotly.\n",
    "mydata = [go.Bar(\n",
    "    x=coeffs['feature'],\n",
    "    y=coeffs['coefficient'],\n",
    "    marker=dict(color=Viridis[::-6])\n",
    ")]\n",
    "\n",
    "mylayout = go.Layout(\n",
    "    title='Married women in 1st class had better odds of survival, especially if younger than 38',\n",
    "    xaxis = {'title': 'Passenger Features'},\n",
    "    yaxis = {'title': 'Odds of Survival'}, \n",
    "\n",
    ")\n",
    "fig = go.Figure(data=mydata, layout=mylayout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(probabilities))\n",
    "print(len(predictions))\n",
    "print(len(y_test))\n",
    "print(len(X_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1=X_test1.reset_index(drop=True)\n",
    "y_test=y_test.reset_index(drop=True)\n",
    "probs=pd.DataFrame(probabilities, columns=['survival_prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back in the names\n",
    "final=pd.concat([X_test1, y_test, probs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('resources/final_probs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata=final.drop(['Survived', 'survival_prob'], axis=1)\n",
    "table=[go.Table(\n",
    "        header=dict(values=list(mydata.columns)),\n",
    "        cells=dict(values=list(mydata.loc[5])))]\n",
    "iplot(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value='Sharp, Mr. Percival James R'\n",
    "survival=final.loc[3, 'survival_prob']\n",
    "survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=df['Name'].values\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs=df['Name'].index.values\n",
    "indexs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=df['Name'].values\n",
    "index=df['Name'].index.values\n",
    "nameslist = list(zip(indexs, names))\n",
    "print(nameslist[5])\n",
    "print(nameslist[5][0])\n",
    "print(nameslist[5][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options=[{'label': k, 'value': i} for i,k in nameslist]\n",
    "options[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value=nameslist[0][0]\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival=final.loc[value, 'survival_prob']\n",
    "round(survival*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on a single, individual row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset=final.drop([\"Survived\", \"survival_prob\", 'Name'], axis=1)\n",
    "firstrow=testset.loc[0]\n",
    "firstrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myarray=firstrow.values\n",
    "myarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisarray=myarray.reshape((1, myarray.shape[0]))\n",
    "thisarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict_proba(thisarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict(thisarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age2028']=np.where((df.age>=20)&(df.age<28))\n",
    "df['age2838']=np.where((df.age>=28)&(df.age<38))\n",
    "df['age3880']=np.where((df.age>=38)&(df.age<80))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
